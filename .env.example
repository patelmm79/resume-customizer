# Resume Customizer - Environment Configuration
# Copy this file to .env and fill in your API keys

# =============================================================================
# GEMINI CONFIGURATION (Google AI)
# =============================================================================
# Get your API key from: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-2.0-flash-exp

# Available Gemini models (defaults):
# - gemini-2.0-flash-exp (recommended, fastest)
# - gemini-1.5-pro (most capable)
# - gemini-1.5-flash (balanced)

# OPTIONAL: Customize available models in dropdown (comma-separated)
# GEMINI_MODELS=gemini-2.0-flash-exp,gemini-1.5-pro,gemini-1.5-flash,gemini-2.0-flash-thinking-exp

# =============================================================================
# CLAUDE CONFIGURATION (Anthropic)
# =============================================================================
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here
CLAUDE_MODEL=claude-3-5-sonnet-20241022

# Available Claude models (defaults):
# - claude-3-5-sonnet-20241022 (recommended, most capable)
# - claude-3-5-haiku-20241022 (fast and efficient)
# - claude-3-opus-20240229 (most powerful)

# OPTIONAL: Customize available models in dropdown (comma-separated)
# CLAUDE_MODELS=claude-3-5-sonnet-20241022,claude-3-5-haiku-20241022,claude-3-opus-20240229

# =============================================================================
# CUSTOM LLM CONFIGURATION (OpenAI-compatible API)
# =============================================================================
# For custom LLMs with OpenAI-compatible API endpoints
# Examples: vLLM containers (Google Cloud Run), OpenAI, Azure OpenAI, LM Studio, Ollama, etc.

# PRIMARY USE CASE: Cloud-hosted vLLM container (e.g., patelmm79/vllm-container-ngc)
# This is the recommended production configuration for this project
CUSTOM_LLM_API_KEY=your_custom_api_key_here
CUSTOM_LLM_BASE_URL=https://your-service-url.run.app/v1
CUSTOM_LLM_MODEL=your-model-name

# OPTIONAL: Customize available models in dropdown (comma-separated)
# Useful when running multiple local models or accessing multiple OpenAI models
# CUSTOM_MODELS=llama3:70b,mixtral:8x7b,qwen2.5:14b,gpt-4-turbo-preview

# Example configurations:
#
# vLLM Container on Google Cloud Run (RECOMMENDED FOR PRODUCTION):
#   CUSTOM_LLM_API_KEY=your-secret-api-key
#   CUSTOM_LLM_BASE_URL=https://your-vllm-service-abc123.run.app/v1
#   CUSTOM_LLM_MODEL=gemma-2-9b-it
#   Note: Uses port 8000 with FastAPI gateway and X-API-Key authentication
#
# OpenAI:
#   CUSTOM_LLM_API_KEY=sk-...
#   CUSTOM_LLM_BASE_URL=https://api.openai.com/v1
#   CUSTOM_LLM_MODEL=gpt-4-turbo-preview
#   CUSTOM_MODELS=gpt-4-turbo-preview,gpt-4,gpt-3.5-turbo
#
# LM Studio (local development):
#   CUSTOM_LLM_API_KEY=lm-studio
#   CUSTOM_LLM_BASE_URL=http://localhost:1234/v1
#   CUSTOM_LLM_MODEL=llama-3.1-8b
#   CUSTOM_MODELS=llama-3.1-8b,mistral-7b,qwen2.5-14b
#
# Ollama (local development):
#   CUSTOM_LLM_API_KEY=ollama
#   CUSTOM_LLM_BASE_URL=http://localhost:11434/v1
#   CUSTOM_LLM_MODEL=llama3:70b
#   CUSTOM_MODELS=llama3:70b,mixtral:8x7b,qwen2.5:14b,gemma2:27b
#
# Azure OpenAI:
#   CUSTOM_LLM_API_KEY=your-azure-key
#   CUSTOM_LLM_BASE_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment
#   CUSTOM_LLM_MODEL=gpt-4

# =============================================================================
# LANGSMITH INTEGRATION (Optional - LLM Tracing)
# =============================================================================
# Get your API key from: https://smith.langchain.com/
# LANGSMITH_TRACING=false
# LANGSMITH_PROJECT=resume-customizer
# LANGSMITH_ENDPOINT=https://api.smith.langchain.com
# LANGSMITH_API_KEY=your_langsmith_api_key_here

# =============================================================================
# LANGFUSE INTEGRATION (Optional - LLM Observability)
# =============================================================================
# Get your keys from: https://cloud.langfuse.com/
# For self-hosted, change LANGFUSE_BASE_URL to your instance URL
# LANGFUSE_ENABLED=false
# LANGFUSE_BASE_URL=https://cloud.langfuse.com
# LANGFUSE_PUBLIC_KEY=your_langfuse_public_key_here
# LANGFUSE_SECRET_KEY=your_langfuse_secret_key_here

# Note: You can enable both LangSmith and Langfuse simultaneously for comparison

# =============================================================================
# NOTES
# =============================================================================
# - You only need to configure the LLM provider(s) you plan to use
# - The application will show configuration status in the sidebar
# - You can switch between providers in the UI dropdown
#
# MULTIPLE MODELS PER PROVIDER:
# - Use GEMINI_MODELS, CLAUDE_MODELS, or CUSTOM_MODELS for custom model lists
# - Format: comma-separated list (e.g., "model1,model2,model3")
# - If not specified, defaults will be used
# - Useful for:
#   * Testing different model versions
#   * Running multiple local models (Ollama/LM Studio)
#   * Accessing different OpenAI model tiers
#   * Custom model deployments
#
# LLM OBSERVABILITY:
# - Both LangSmith and Langfuse are optional and can be used together
# - Enable for production deployments to track LLM performance and costs
# - Useful for debugging, monitoring, and analyzing LLM behavior
